# 10-4. The Least Squares Regression Line

### Goodness of Fit of a Straight Line to Data

Once the scatter diagram of the data has been drawn and the model assumptions described in the previous sections at least visually verified \(and perhaps the correlation coefficient _r_ computed to quantitatively verify the linear trend\), the next step in the analysis is to find the straight line that best fits the data. We will explain how to measure how well a straight line fits a collection of points by examining how well the line y=12x−1y=12x−1 fits the data setxy20216283103x226810y01233

\(which will be used as a running example for the next three sections\). We will write the equation of this line as yˆ=12x−1y^=12x−1 with an accent on the _y_ to indicate that the _y_-values computed using this equation are not from the data. We will do this with all lines approximating data sets. The line yˆ=12x−1y^=12x−1 was selected as one that seems to fit the data reasonably well.

The idea for measuring the goodness of fit of a straight line to data is illustrated in [Figure 10.6 "Plot of the Five-Point Data and the Line "](https://saylordotorg.github.io/text_introductory-statistics/s14-correlation-and-regression.html#fwk-shafer-ch10_s04_s01_f01), in which the graph of the line yˆ=12x−1y^=12x−1 has been superimposed on the scatter plot for the sample data set.

Figure 10.6 Plot of the Five-Point Data and the Line yˆ=12x−1y^=12x−1![](https://saylordotorg.github.io/text_introductory-statistics/section_14/2f707e3177c1a3f56b84fb8ded3894aa.jpg)

To each point in the data set there is associated an “error,” the positive or negative vertical distance from the point to the line: positive if the point is above the line and negative if it is below the line. The error can be computed as the actual _y_-value of the point minus the _y_-value yˆy^ that is “predicted” by inserting the _x_-value of the data point into the formula for the line:error at data point \(x,y\)=\(true y\)−\(predicted y\)=y−yˆerror at data point \(x,y\)=\(true y\)−\(predicted y\)=y−y^

The computation of the error for each of the five points in the data set is shown in [Table 10.1 "The Errors in Fitting Data with a Straight Line"](https://saylordotorg.github.io/text_introductory-statistics/s14-correlation-and-regression.html#fwk-shafer-ch10_s04_s01_t01).

Table 10.1 The Errors in Fitting Data with a Straight Line

|  | _x_ | _y_ | yˆ=12x−1y^=12x−1 | y−yˆy−y^ | \(y−yˆ\)2\(y−y^\)2 |
| :--- | :--- | :--- | :--- | :--- | :--- |
|  | 2 | 0 | 0 | 0 | 0 |
|  | 2 | 1 | 0 | 1 | 1 |
|  | 6 | 2 | 2 | 0 | 0 |
|  | 8 | 3 | 3 | 0 | 0 |
|  | 10 | 3 | 4 | −1 | 1 |
| Σ | - | - | - | 0 | 2 |

A first thought for a measure of the goodness of fit of the line to the data would be simply to add the errors at every point, but the example shows that this cannot work well in general. The line does not fit the data perfectly \(no line can\), yet because of cancellation of positive and negative errors the sum of the errors \(the fourth column of numbers\) is zero. Instead goodness of fit is measured by the sum of the squares of the errors. Squaring eliminates the minus signs, so no cancellation can occur. For the data and line in [Figure 10.6 "Plot of the Five-Point Data and the Line "](https://saylordotorg.github.io/text_introductory-statistics/s14-correlation-and-regression.html#fwk-shafer-ch10_s04_s01_f01) the sum of the squared errors \(the last column of numbers\) is 2. This number measures the goodness of fit of the line to the data.

#### Definition

_The_ **goodness of fit** _of a line_ yˆ=mx+by^=mx+b _to a set of_ _n_ _pairs_ \(x,y\)\(x,y\) _of numbers in a sample is the sum of the squared errors_Σ\(y−yˆ\)2Σ\(y−y^\)2

_\(n_ _terms in the sum, one for each data pair\)._

### The Least Squares Regression Line

Given any collection of pairs of numbers \(except when all the _x_-values are the same\) and the corresponding scatter diagram, there always exists exactly one straight line that fits the data better than any other, in the sense of minimizing the sum of the squared errors. It is called the _least squares regression line_. Moreover there are formulas for its slope and _y_-intercept.

#### Definition

_Given a collection of pairs_ \(x,y\)\(x,y\) _of numbers \(in which not all the_ _x-values are the same\), there is a line_ yˆ=βˆ1x+βˆ0y^=β^1x+β^0 _that best fits the data in the sense of minimizing the sum of the squared errors. It is called the_ least squares regression line. _Its slope_ βˆ1β^1 _and_ _y-intercept_ βˆ0β^0 _are computed using the formulas_βˆ1=SSxySSxx and βˆ0=y–−βˆ1x−−β^1=SSxySSxx and β^0=y-−β^1x-

_where_SSxx=Σx2−1n\(Σx\)2, SSxy=Σxy−1n\(Σx\)\(Σy\)SSxx=Σx2−1n\(Σx\)2, SSxy=Σxy−1n\(Σx\)\(Σy\)

x−−x- _is the mean of all the_ _x-values,_ y–y- _is the mean of all the_ _y-values, and_ _n_ _is the number of pairs in the data set._

_The equation_ yˆ=βˆ1x+βˆ0y^=β^1x+β^0 _specifying the least squares regression line is called the_ least squares regression equation.

Remember from [Section 10.3 "Modelling Linear Relationships with Randomness Present"](https://saylordotorg.github.io/text_introductory-statistics/s14-03-modelling-linear-relationships.html) that the line with the equation y=β1x+β0y=β1x+β0 is called the population regression line. The numbers βˆ1β^1 and βˆ0β^0 are statistics that estimate the population parameters β1β1 and β0.β0.

We will compute the least squares regression line for the five-point data set, then for a more practical example that will be another running example for the introduction of new concepts in this and the next three sections.

#### EXAMPLE 2

Find the least squares regression line for the five-point data setxy20216283103x226810y01233

and verify that it fits the data better than the line yˆ=12x−1y^=12x−1 considered in [Section 10.4.1 "Goodness of Fit of a Straight Line to Data"](https://saylordotorg.github.io/text_introductory-statistics/s14-correlation-and-regression.html#fwk-shafer-ch10_s04_s01).

Solution:











TABLE 10.2 THE ERRORS IN FITTING DATA WITH THE LEAST SQUARES REGRESSION LINE

| _x_ | _y_ | yˆ=0.34375x−0.125y^=0.34375x−0.125 | y−yˆy−y^ | \(y−yˆ\)2\(y−y^\)2 |
| :--- | :--- | :--- | :--- | :--- |
| 2 | 0 | 0.5625 | −0.5625 | 0.31640625 |
| 2 | 1 | 0.5625 | 0.4375 | 0.19140625 |
| 6 | 2 | 1.9375 | 0.0625 | 0.00390625 |
| 8 | 3 | 2.6250 | 0.3750 | 0.14062500 |
| 10 | 3 | 3.3125 | −0.3125 | 0.09765625 |

#### 

#### EXAMPLE 3

[Table 10.3 "Data on Age and Value of Used Automobiles of a Specific Make and Model"](https://saylordotorg.github.io/text_introductory-statistics/s14-correlation-and-regression.html#fwk-shafer-ch10_s04_s02_t03) shows the age in years and the retail value in thousands of dollars of a random sample of ten automobiles of the same make and model.

1. Construct the scatter diagram.
2. Compute the linear correlation coefficient _r_. Interpret its value in the context of the problem.
3. Compute the least squares regression line. Plot it on the scatter diagram.
4. Interpret the meaning of the slope of the least squares regression line in the context of the problem.
5. Suppose a four-year-old automobile of this make and model is selected at random. Use the regression equation to predict its retail value.
6. Suppose a 20-year-old automobile of this make and model is selected at random. Use the regression equation to predict its retail value. Interpret the result.
7. Comment on the validity of using the regression equation to predict the price of a brand new automobile of this make and model.

TABLE 10.3 DATA ON AGE AND VALUE OF USED AUTOMOBILES OF A SPECIFIC MAKE AND MODEL

| _x_ | 2 | 3 | 3 | 3 | 4 | 4 | 5 | 5 | 5 | 6 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| _y_ | 28.7 | 24.8 | 26.0 | 30.5 | 23.8 | 24.6 | 23.8 | 20.4 | 21.6 | 22.1 |



For emphasis we highlight the points raised by parts \(f\) and \(g\) of the example.

#### Definition

_The process of using the least squares regression equation to estimate the value of_ _y_ _at a value of_ _x_ _that does not lie in the range of the_ _x-values in the data set that was used to form the regression line is called_ extrapolation_. It is an invalid use of the regression equation that can lead to errors, hence should be avoided._

### The Sum of the Squared Errors SSESSE

In general, in order to measure the goodness of fit of a line to a set of data, we must compute the predicted _y_-value yˆy^ at every point in the data set, compute each error, square it, and then add up all the squares. In the case of the least squares regression line, however, the line that best fits the data, the sum of the squared errors can be computed directly from the data using the following formula.

The sum of the squared errors for the least squares regression line is denoted by SSE.SSE. It can be computed using the formulaSSE=SSyy−βˆ1SSxySSE=SSyy−β^1SSxy

#### EXAMPLE 4

Find the sum of the squared errors SSESSE for the least squares regression line for the five-point data setxy20216283103x226810y01233

Do so in two ways:

1. using the definition Σ\(y−yˆ\)2Σ\(y−y^\)2;
2. using the formula SSE=SSyy−βˆ1SSxy.SSE=SSyy−β^1SSxy.

Solution:



#### EXAMPLE 5

Find the sum of the squared errors SSESSE for the least squares regression line for the data set, presented in [Table 10.3 "Data on Age and Value of Used Automobiles of a Specific Make and Model"](https://saylordotorg.github.io/text_introductory-statistics/s14-correlation-and-regression.html#fwk-shafer-ch10_s04_s02_t03), on age and values of used vehicles in [Note 10.19 "Example 3"](https://saylordotorg.github.io/text_introductory-statistics/s14-04-the-least-squares-regression-l.html).

Solution:





