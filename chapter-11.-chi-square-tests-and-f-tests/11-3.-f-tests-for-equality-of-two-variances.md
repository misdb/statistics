# 11-3. F-tests for Equality of Two Variances

## 1. _F_-Distributions

Another important and useful family of distributions in statistics is the family of _F_-distributions. Each member of the _F_-distribution family is specified by a pair of parameters called _degrees of freedom_ and denoted $$df_1$$ and $$df_2$$ . [Figure 11.7 "Many "](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s03_s01_f01) shows several _F_-distributions for different pairs of degrees of freedom. An **F random variable** is a random variable that assumes only positive values and follows an _F_-distribution.

Figure 11.7 Many _F_-Distributions

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/75fd6a2d869b9403de8408b42a054db9.jpg)

The parameter $$df_1$$ is often referred to as the _numerator_ degrees of freedom and the parameter $$df_2$$ as the _denominator_ degrees of freedom. It is important to keep in mind that they are not interchangeable. For example, the _F_-distribution with degrees of freedom $$df_1=3$$ and $$df_2=8$$ is a different distribution from the _F_-distribution with degrees of freedom $$df_1=8$$ and $$df_2=3$$.

#### 

> _The value of the_ _F random variable_ _F_ _with degrees of freedom_ $$df_1$$ _and_ $$df_2$$ _that cuts off a right tail of area_ _c_ _is denoted_ $$F_c$$ _and is called a_ **critical value**. _See_ [_Figure 11.8_](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s03_s01_f02)_._
>
> Figure 11.8 $$F_c$$ Illustrated

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/59a41857cbaae56b820472577df06b26.jpg)

Tables containing the values of $$F_c$$ are given in [Chapter 11 "Chi-Square Tests and "](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html). Each of the tables is for a fixed collection of values of _c_, either 0.900, 0.950, 0.975, 0.990, and 0.995 \(yielding what are called “lower” critical values\), or 0.005, 0.010, 0.025, 0.050, and 0.100 \(yielding what are called “upper” critical values\). In each table critical values are given for various pairs $$(df_1,df_2)$$ __. We illustrate the use of the tables with several examples.



### [F-Distribution Tables](http://www.socr.ucla.edu/Applets.dir/F_Table.html#FTable0.05)



**EXAMPLE 3.**  Suppose _F_ is an _F_ random variable with degrees of freedom $$df_1=5$$ and $$df_2=4$$ . Use the tables to find

1. $$F_{0.10}$$ 
2. $$F_{0.95}$$ 

**\[ Solution \]**



1. The column headings of all the tables contain $$df_1=5$$ . Look for the table for which 0.10 is one of the entries on the extreme left \(a table of upper critical values\) and that has a row heading $$df_2=4$$ in the left margin of the table. A portion of the relevant table is provided. The entry in the intersection of the column with heading $$df_1=5$$ and the row with the headings 0.10 and $$df_2=4$$, which is shaded in the table provided, is the answer, $$F_{0.10}=4.05$$ .

   | _F_ Tail Area | $$df_1$$ | 1 | 2 |  ⋅ ⋅ ⋅  · · ·  | 5 |  ⋅ ⋅ ⋅  · · ·  |
   | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
   | $$df_2$$ |  |  |  |  |  |  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |
   | 0.005 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | 22.5 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.01 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | 15.5 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.025 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | 9.36 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.05 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | 6.26 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.10 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | $$4.05$$  |  ⋅ ⋅ ⋅  · · ·  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |

2. Look for the table for which 0.95 is one of the entries on the extreme left \(a table of lower critical values\) and that has a row heading $$df_2=4$$ in the left margin of the table. A portion of the relevant table is provided. The entry in the intersection of the column with heading $$df_1=5$$ and the row with the headings 0.95 and $$df_2=4$$, which is shaded in the table provided, is the answer, $$F_{0.95}=0.19$$ .

   | _F_ Tail Area | $$df_1$$ | 1 | 2 |  ⋅ ⋅ ⋅  · · ·  | 5 |  ⋅ ⋅ ⋅  · · ·  |
   | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
   | $$df_2$$ |  |  |  |  |  |  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |
   | 0.90 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | 0.28 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.95 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | $$0.19$$  |  ⋅ ⋅ ⋅  · · ·  |
   | 0.975 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | 0.14 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.99 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | 0.09 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.995 | 4 |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  |  ⋅ ⋅ ⋅  · · ·  | 0.06 |  ⋅ ⋅ ⋅  · · ·  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |



{% tabs %}
{% tab title="R Source" %}
```
library(Rstat)

df_1 <- 5
df_2 <- 4

pv <- c(0.90, 0.05)
f.quant(df_1, df_2, pv)
```
{% endtab %}

{% tab title="Result" %}
```
> f.quant(df_1, df_2, pv)
##   0.9   0.05 
## 4.0506 0.1926 
```

![](../.gitbook/assets/image%20%2898%29.png)
{% endtab %}
{% endtabs %}



**EXAMPLE 4.**   Suppose $$F$$ is an _F_ random variable with degrees of freedom $$df_1=2$$ and $$df_2=20$$. Let $$α=0.05$$ . Use the tables to find

1. $$F_α$$ 
2. $$F_{α∕2}$$ 
3. $$F_{1−α}$$ 
4. $$F_{1−α∕2}$$ 

**\[ Solution \]**

1. The column headings of all the tables contain $$ df_1=2$$ . Look for the table for which $$α=0.05$$ is one of the entries on the extreme left \(a table of upper critical values\) and that has a row heading $$df_2=20$$ in the left margin of the table. A portion of the relevant table is provided. The shaded entry, in the intersection of the column with heading $$ df_1=2$$ and the row with the headings 0.05 and $$df_2=20$$ is the answer, $$F_{0.05}=3.49$$ .

   | _F_ Tail Area | $$df_1$$ | 1 | 2 |  ⋅ ⋅ ⋅  · · ·  |
   | :--- | :--- | :--- | :--- | :--- |
   | $$df_2$$ |  |  |  |  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |
   | 0.005 | 20 |  ⋅ ⋅ ⋅  · · ·  | 6.99 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.01 | 20 |  ⋅ ⋅ ⋅  · · ·  | 5.85 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.025 | 20 |  ⋅ ⋅ ⋅  · · ·  | 4.46 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.05 | 20 |  ⋅ ⋅ ⋅  · · ·  | $$3.49$$  |  ⋅ ⋅ ⋅  · · ·  |
   | 0.10 | 20 |  ⋅ ⋅ ⋅  · · ·  | 2.59 |  ⋅ ⋅ ⋅  · · ·  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |

2. Look for the table for which $$α∕2=0.025$$ is one of the entries on the extreme left \(a table of upper critical values\) and that has a row heading $$df_2=20$$ in the left margin of the table. A portion of the relevant table is provided. The shaded entry, in the intersection of the column with heading $$ df_1=2$$ and the row with the headings 0.025 and $$df_2=20$$ is the answer, $$ F_{0.025}=4.46$$ .

   | _F_ Tail Area | $$df_1$$ | 1 | 2 |  ⋅ ⋅ ⋅  · · ·  |
   | :--- | :--- | :--- | :--- | :--- |
   | $$df_2$$ |  |  |  |  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |
   | 0.005 | 20 |  ⋅ ⋅ ⋅  · · ·  | 6.99 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.01 | 20 |  ⋅ ⋅ ⋅  · · ·  | 5.85 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.025 | 20 |  ⋅ ⋅ ⋅  · · ·  | $$4.46$$  |  ⋅ ⋅ ⋅  · · ·  |
   | 0.05 | 20 |  ⋅ ⋅ ⋅  · · ·  | 3.49 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.10 | 20 |  ⋅ ⋅ ⋅  · · ·  | 2.59 |  ⋅ ⋅ ⋅  · · ·  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |

3. Look for the table for which $$(1−α)=0.95$$ is one of the entries on the extreme left \(a table of lower critical values\) and that has a row heading $$df_2=20$$ in the left margin of the table. A portion of the relevant table is provided. The shaded entry, in the intersection of the column with heading $$ df_1=2$$ and the row with the headings 0.95 and $$df_2=20$$ is the answer, F0.95=0.05.F0.95=0.05.

   | _F_ Tail Area | $$df_1$$ | 1 | 2 |  ⋅ ⋅ ⋅  · · ·  |
   | :--- | :--- | :--- | :--- | :--- |
   | $$df_2$$ |  |  |  |  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |
   | 0.90 | 20 |  ⋅ ⋅ ⋅  · · ·  | 0.11 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.95 | 20 |  ⋅ ⋅ ⋅  · · ·  | $$0.05$$  |  ⋅ ⋅ ⋅  · · ·  |
   | 0.975 | 20 |  ⋅ ⋅ ⋅  · · ·  | 0.03 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.99 | 20 |  ⋅ ⋅ ⋅  · · ·  | 0.01 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.995 | 20 |  ⋅ ⋅ ⋅  · · ·  | 0.01 |  ⋅ ⋅ ⋅  · · ·  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |

4. Look for the table for which $$(1−α∕2)=0.975$$ is one of the entries on the extreme left \(a table of lower critical values\) and that has a row heading $$df_2=20$$ in the left margin of the table. A portion of the relevant table is provided. The shaded entry, in the intersection of the column with heading $$ df_1=2$$ and the row with the headings 0.975 and $$df_2=20$$ is the answer, $$F_{0.975}=0.03$$ .

   | _F_ Tail Area | $$df_1$$ | 1 | 2 |  ⋅ ⋅ ⋅  · · ·  |
   | :--- | :--- | :--- | :--- | :--- |
   | $$df_2$$ |  |  |  |  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |
   | 0.90 | 20 |  ⋅ ⋅ ⋅  · · ·  | 0.11 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.95 | 20 |  ⋅ ⋅ ⋅  · · ·  | 0.05 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.975 | 20 |  ⋅ ⋅ ⋅  · · ·  | $$0.03$$  |  ⋅ ⋅ ⋅  · · ·  |
   | 0.99 | 20 |  ⋅ ⋅ ⋅  · · ·  | 0.01 |  ⋅ ⋅ ⋅  · · ·  |
   | 0.995 | 20 |  ⋅ ⋅ ⋅  · · ·  | 0.01 |  ⋅ ⋅ ⋅  · · ·  |
   | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |

{% tabs %}
{% tab title="R Source" %}
```
library(Rstat)

df_1 <- 2
df_2 <- 20

pv <- c(0.95, 0.975, 0.05, 0.025)
f.quant(df_1, df_2, pv)

```
{% endtab %}

{% tab title="Result" %}
```
> f.quant(df_1, df_2, pv)
##   0.95  0.975   0.05  0.025 
## 3.4928 4.4613 0.0514 0.0253 
```

![](../.gitbook/assets/image%20%28179%29.png)
{% endtab %}
{% endtabs %}

> A fact that sometimes allows us to find a critical value from a table that we could not read otherwise is:
>
> If $$F_u(r,s)$$ denotes the value of the _F_-distribution with degrees of freedom $$df_1=r$$ and $$df_2=s$$ 
>
> that cuts off a right tail of area _u_, then
>
>                                                                $$F_c(k,ℓ)= \frac {1}{F_{1−c}(ℓ,k)}$$



**EXAMPLE 5.**  Use the tables to find

1. $$F_{0.01}$$ for an _F_ random variable with $$df_1=13$$ and $$df_2=8$$ 
2. $$F_{0.975}$$ for an _F_ random variable with $$df_1=40$$ and $$df_2=10$$ 

**\[ Solution \]**

1. There is no table with $$df_1=13$$, but there is one with $$df_2=8$$. Thus we use the fact that                                $$F_{0.01}(13, 8)= \frac{1}{F_{0.99}(8,13)}$$  

Using the relevant table we find that $$F_{0.99}(8,13)=0.18$$ ,   
hence  $$F_{0.01}(13,8)=0.18^{-1} = 5.556.$$  


{% tabs %}
{% tab title="R Source" %}
```
library(Rstat)

df_1 <- 13
df_2 <- 8
alpha <- 0.01

pv <- c(1-alpha)
f.quant(df_1, df_2, pv)

pv_1 <- c(alpha)
f.quant(df_2, df_1, pv_1)

```
{% endtab %}

{% tab title="Result" %}
```
> pv <- c(1-alpha)
> f.quant(df_1, df_2, pv)
##   0.99 
## 5.6089 
> pv_1 <- c(alpha)
> f.quant(df_2, df_1, pv_1) 
##   0.01 
## 0.1783 
```

![](../.gitbook/assets/image%20%28111%29.png)

![](../.gitbook/assets/image%20%28153%29.png)
{% endtab %}
{% endtabs %}



2. There is no table with $$df_1=40$$, but there is one with $$df_2=10$$. Thus we use the fact that  
  
                                $$F_{0.975}(40,10)= \frac{1}{F_{0.025}(10,40)}$$

  
Using the relevant table we find that $$F_{0.025}(10,40)=2.3882$$ ,   
hence $$F_{0.975}(10,40)=2.3882^{-1}=0.4187$$

{% tabs %}
{% tab title="R Source" %}
```
library(Rstat)

df_1 <- 40
df_2 <- 10
alpha <- 0.975

pv <- c(1-alpha)
f.quant(df_1, df_2, pv)

pv_1 <- c(alpha)
f.quant(df_2, df_1, pv_1)


```
{% endtab %}

{% tab title="Result" %}
```
> pv <- c(1-alpha)
> f.quant(df_1, df_2, pv)
##  0.025 
## 0.4187 
> pv_1 <- c(alpha)
> f.quant(df_2, df_1, pv_1)
##  0.975 
## 2.3882 
```

![](../.gitbook/assets/image%20%28139%29.png)

![](../.gitbook/assets/image%20%2890%29.png)
{% endtab %}
{% endtabs %}



## 2. _F_-Tests for Equality of Two Variances

In [Chapter 9 "Two-Sample Problems"](https://saylordotorg.github.io/text_introductory-statistics/s13-two-sample-problems.html) we saw how to test hypotheses about the difference between two population means $$μ_1$$ and $$μ_2$$ . In some practical situations the difference between the population standard deviations $$σ_1$$ and $$σ_2$$ is also of interest. Standard deviation measures the variability of a random variable. For example, if the random variable measures the size of a machined part in a manufacturing process, the size of standard deviation is one indicator of product quality. A smaller standard deviation among items produced in the manufacturing process is desirable since it indicates consistency in product quality.

For theoretical reasons it is easier to compare the squares of the population standard deviations, the population variances $$σ_1^2$$ and $$σ_2^2$$. This is not a problem, since $$σ_1=σ_2$$ precisely when $$σ^2_1=σ_2^2$$ , $$σ_1<σ_2$$ precisely when $$σ_1^2<σ_2^2$$ , and $$σ_1>σ_2$$ precisely when $$σ_1^2>σ_2^2$$ .

The null hypothesis always has the form $$H_0:σ_1=σ_2$$. The three forms of the alternative hypothesis, with the terminology for each case, are:

| Form of $$H_a$$  | Terminology |
| :--- | :--- |
| $$H_a:σ_1^2>σ_2^2$$ | Right-tailed |
| $$H_a:σ_1^2<σ_2^2$$ | Left-tailed |
| $$H_a:σ_1^2 \ne σ_2^2$$ | Two-tailed |

Just as when we test hypotheses concerning two population means, we take a random sample from each population, of sizes $$n_1$$ and $$n_2$$ , and compute the sample standard deviations $$s_1$$ and $$s_2$$ . In this context the samples are always independent. The populations themselves must be normally distributed.

> **Test Statistic for Hypothesis Tests Concerning the Difference Between Two Population Variances**
>
>                                                             $$F= \frac{s^2_1}{s_2^2}$$ 
>
> If the two populations are normally distributed and if  $$H_0:σ^2_1=σ_2^2$$  is true then under independent sampling _F_ approximately follows an _F_-distribution with degrees of freedom $$df_1=(n_1−1)$$ and $$df_2=(n_2−1)$$ .



A test based on the test statistic $$F$$ is called an _F_-test.

A most important point is that while the rejection region for a right-tailed test is exactly as in every other situation that we have encountered, because of the asymmetry in the _F_-distribution the critical value for a left-tailed test and the lower critical value for a two-tailed test have the special forms shown in the following table:

| Terminology | Alternative Hypothesis | Rejection Region |
| :--- | :--- | :--- |
| Right-tailed | $$H_a:σ_1^2>σ_2^2$$ | $$F≥F_α$$  |
| Left-tailed | $$H_a:σ_1^2<σ_2^2$$ | $$F≤F_{1−α}$$  |
| Two-tailed | $$H_a:σ_1^2 \ne σ_2^2$$ | $$ F≤F_{1−α∕2} \space or \space F≥F_{α∕2}$$  |

[Figure 11.9 "Rejection Regions: \(a\) Right-Tailed; \(b\) Left-Tailed; \(c\) Two-Tailed"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s03_s02_f01) illustrates these **rejection regions**.

Figure 11.9  Rejection Regions: \(a\) Right-Tailed; \(b\) Left-Tailed; \(c\) Two-Tailed

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/da58e4f2977f4b9644c6f0c320af4429.jpg)

The test is performed using the usual five-step procedure described at the end of [Section 8.1 "The Elements of Hypothesis Testing"](https://saylordotorg.github.io/text_introductory-statistics/fwk-shafer-ch08_s01#fwk-shafer-ch08_s01) in [Chapter 8 "Testing Hypotheses"](https://saylordotorg.github.io/text_introductory-statistics/s12-testing-hypotheses.html).



**EXAMPLE 6.**  One of the quality measures of blood glucose meter strips is the consistency of the test results on the same sample of blood. The consistency is measured by the variance of the readings in repeated testing. Suppose two types of strips, _A_ and _B_, are compared for their respective consistencies. We arbitrarily label the population of Type _A_ strips Population 1 and the population of Type _B_ strips Population 2. Suppose 15 Type _A_ strips were tested with blood drops from a well-shaken vial and 20 Type _B_ strips were tested with the blood from the same vial. The results are summarized in [Table 11.16 "Two Types of Test Strips"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s03_s02_t01). Assume the glucose readings using Type _A_ strips follow a normal distribution with variance σ21σ12 and those using Type _B_ strips follow a normal distribution with variance with σ22.σ22. Test, at the 10% level of significance, whether the data provide sufficient evidence to conclude that the consistencies of the two types of strips are different.

TABLE 11.16 TWO TYPES OF TEST STRIPS

| Strip Type | Sample Size | Sample Variance |
| :--- | :--- | :--- |
| _A_ | $$n_1=16$$  | $$s_1^2=2.09$$  |
| _B_ | $$n_2=21$$  | $$s_2^2=1.10$$  |

**\[ Solution \]**

* **Step 1.** The test of **hypotheses** is                       $$H_0:σ_1=σ_2$$                vs. $$H_a:σ_1^2 \ne σ_2^2   \space \space @ α=0.10$$ 
* **Step 2.** The distribution is the _F_-distribution with **degrees of freedom**

                $$df_1=(16−1)=15$$ and $$df_2=(21−1)=20$$ .  

* **Step 3.** The test is **two-tailed**. The **left or lower critical value** is                                 $$F_{1−α∕2}=F_{0.95}=0.43$$ .              The right or upper critical value is                                   $$F_{α∕2}=F_{0.05}=2.20$$ .               Thus the **rejection region** is $$[0,−0.43]∪[2.20,∞)$$ ,               as illustrated in [Figure 11.10 "Rejection Region and Test Statistic for "](https://saylordotorg.github.io/text_introductory-statistics/s15-03-f-tests-for-equality-of-two-va.html#fwk-shafer-ch11_s03_s02_f02).

Figure 11.10  Rejection Region and Test Statistic for [Note 11.27 "Example 6"](https://saylordotorg.github.io/text_introductory-statistics/s15-03-f-tests-for-equality-of-two-va.html#fwk-shafer-ch11_s03_s02_n02)

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/5529f95c575a61746d7c6db537493c0f.jpg)

* **Step 4.** The value of the test statistic is                 $$F=\frac{s^2_1}{s^2_2}=\frac{2.09}{1.10}=1.90$$  
* **Step 5.** As shown in [Figure 11.10 "Rejection Region and Test Statistic for "](https://saylordotorg.github.io/text_introductory-statistics/s15-03-f-tests-for-equality-of-two-va.html#fwk-shafer-ch11_s03_s02_f02),                the test statistic 1.90 does not lie in the rejection region, so the decision is **not to reject** $$H_0$$ .  
* The data do not provide sufficient evidence, at the 10% level of significance, to **conclude** that there is a difference in the consistency, as measured by the variance, of the two types of test strips.



**EXAMPLE 7.**  In the context of [Note 11.27 "Example 6"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s03_s02_n02), suppose Type _A_ test strips are the current market leader and Type _B_ test strips are a newly improved version of Type _A_. Test, at the 10% level of significance, whether the data given in [Table 11.16 "Two Types of Test Strips"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s03_s02_t01) provide sufficient evidence to conclude that Type _B_ test strips have better consistency \(lower variance\) than Type _A_ test strips.

**\[ Solution \]**

* **Step 1.** The test of **hypotheses** is now                       $$H_0:σ_1=σ_2$$                vs. $$H_a:σ_1^2 \ne σ_2^2   \space \space @ α=0.10$$ 
* **Step 2.** The distribution is the ****_**F**_**-distribution** with degrees of freedom   
             

               $$df_1=(16−1)=15$$ and $$df_2=(21−1)=20$$

* **Step 3.** The **value of the test statistic** is  
  
                           $$F=\frac{s^2_1}{s^2_2}=\frac{2.09}{1.10}=1.90$$

* **Step 4.** The test is right-tailed. The **single critical value** is   
  
               $$F_α=F_{0.10}=1.84$$

  
               Thus the **rejection region** is $$[1.84,∞)$$,   
               as illustrated in [Figure 11.11 "Rejection Region and Test Statistic for "](https://saylordotorg.github.io/text_introductory-statistics/s15-03-f-tests-for-equality-of-two-va.html#fwk-shafer-ch11_s03_s02_f03).

Figure 11.11 Rejection Region and Test Statistic for [Note 11.28 "Example 7"](https://saylordotorg.github.io/text_introductory-statistics/s15-03-f-tests-for-equality-of-two-va.html#fwk-shafer-ch11_s03_s02_n03)

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/0dee7b0964c04670ae90832291b81e85.jpg)

* **Step 5.** As shown in [Figure 11.11 "Rejection Region and Test Statistic for "](https://saylordotorg.github.io/text_introductory-statistics/s15-03-f-tests-for-equality-of-two-va.html#fwk-shafer-ch11_s03_s02_f03), the test statistic 1.90 lies in the rejection region, so the decision is **to reject** $$H_0$$ .  
* The data provide sufficient evidence, at the 10% level of significance, to **conclude** that Type _B_ test strips have better consistency \(lower variance\) than Type _A_ test strips do.





