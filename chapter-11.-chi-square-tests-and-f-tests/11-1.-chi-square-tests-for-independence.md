# 11-1. Chi-Square Tests for Independence

In previous chapters you saw how to test hypotheses concerning population means and population proportions. The idea of testing hypotheses can be extended to many other situations that involve different parameters and use different test statistics. Whereas the standardized test statistics that appeared in earlier chapters followed either a normal or Student _t_-distribution, in this chapter the tests will involve two other very common and useful distributions, the chi-square and the _F_-distributions. The chi-square distribution arises in tests of hypotheses concerning the independence of two random variables and concerning whether a discrete random variable follows a specified distribution. The _F_-distribution arises in tests of hypotheses concerning whether or not two population variances are equal and concerning whether or not three or more population means are equal.

### 11.1 Chi-Square Tests for Independence

#### LEARNING OBJECTIVES

1. To understand what chi-square distributions are.
2. To understand how to use a chi-square test to judge whether two factors are independent.

### Chi-Square Distributions

As you know, there is a whole family of _t_-distributions, each one specified by a parameter called the _degrees of freedom_, denoted df.df. Similarly, all the chi-square distributions form a family, and each of its members is also specified by a parameter dfdf, the number of degrees of freedom. Chi is a Greek letter denoted by the symbol χχ and chi-square is often denoted by χ2.χ2. [Figure 11.1 "Many "](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s01_f01) shows several chi-square distributions for different degrees of freedom. A chi-square random variable is a random variable that assumes only positive values and follows a chi-square distribution.

Figure 11.1 Many χ2χ2 Distributions![](https://saylordotorg.github.io/text_introductory-statistics/section_15/5a0c7bbacb4242555e8a85c9767c03ee.jpg)

#### Definition

_The value of the chi-square random variable_ χ2χ2 _with_ df=kdf=k _that cuts off a right tail of area_ _c_ _is denoted_ χ2cχc2 _and is called a_ **critical value**. _See_ [_Figure 11.2_](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s01_f02)_._

Figure 11.2χ2cχc2 Illustrated![](https://saylordotorg.github.io/text_introductory-statistics/section_15/34d06306c2e726f6d5cd7479d9736e5e.jpg)

[Figure 12.4 "Critical Values of Chi-Square Distributions"](https://saylordotorg.github.io/text_introductory-statistics/s16-appendix.html) gives values of χ2cχc2 for various values of _c_ and under several chi-square distributions with various degrees of freedom.

### Tests for Independence

Hypotheses tests encountered earlier in the book had to do with how the numerical values of two population parameters compared. In this subsection we will investigate hypotheses that have to do with whether or not two random variables take their values independently, or whether the value of one has a relation to the value of the other. Thus the hypotheses will be expressed in words, not mathematical symbols. We build the discussion around the following example.

There is a theory that the gender of a baby in the womb is related to the baby’s heart rate: baby girls tend to have higher heart rates. Suppose we wish to test this theory. We examine the heart rate records of 40 babies taken during their mothers’ last prenatal checkups before delivery, and to each of these 40 randomly selected records we compute the values of two random measures: 1\) gender and 2\) heart rate. In this context these two random measures are often called factors. Since the burden of proof is that heart rate and gender are related, not that they are unrelated, the problem of testing the theory on baby gender and heart rate can be formulated as a test of the following hypotheses:H0vs. Ha::Baby gender and baby heart rate are independentBaby gender and baby heart rate are not independentH0:Baby gender and baby heart rate are independentvs. Ha:Baby gender and baby heart rate are not independent

The factor gender has two natural categories or levels: boy and girl. We divide the second factor, heart rate, into two levels, low and high, by choosing some heart rate, say 145 beats per minute, as the cutoff between them. A heart rate below 145 beats per minute will be considered low and 145 and above considered high. The 40 records give rise to a 2 × 2 _contingency table_. By adjoining row totals, column totals, and a grand total we obtain the table shown as [Table 11.1 "Baby Gender and Heart Rate"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t01). The four entries in boldface type are counts of observations from the sample of _n_ = 40. There were 11 girls with low heart rate, 17 boys with low heart rate, and so on. They form the _core_ of the expanded table.

Table 11.1 Baby Gender and Heart Rate

|  | Heart Rate |  |  |  |
| :--- | :--- | :--- | :--- | :--- |
| Low | High | Row Total |  |  |
| Gender | Girl | **11** | **7** | 18 |
| Boy | **17** | **5** | 22 |  |
| Column Total | 28 | 12 | Total = 40 |  |

In analogy with the fact that the probability of independent events is the product of the probabilities of each event, if heart rate and gender were independent then we would expect the number in each core cell to be close to the product of the row total _R_ and column total _C_ of the row and column containing it, divided by the sample size _n_. Denoting such an expected number of observations _E_, these four expected values are:

* 1st row and 1st column: E=\(R×C\)∕n=18×28∕40=12.6E=\(R×C\)∕n=18×28∕40=12.6
* 1st row and 2nd column: E=\(R×C\)∕n=18×12∕40=5.4E=\(R×C\)∕n=18×12∕40=5.4
* 2nd row and 1st column: E=\(R×C\)∕n=22×28∕40=15.4E=\(R×C\)∕n=22×28∕40=15.4
* 2nd row and 2nd column: E=\(R×C\)∕n=22×12∕40=6.6E=\(R×C\)∕n=22×12∕40=6.6

We update [Table 11.1 "Baby Gender and Heart Rate"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t01) by placing each expected value in its corresponding core cell, right under the observed value in the cell. This gives the updated table [Table 11.2 "Updated Baby Gender and Heart Rate"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t02).

Table 11.2 Updated Baby Gender and Heart Rate

|  | Heart Rate |  |  |  |
| :--- | :--- | :--- | :--- | :--- |
| Low | High | Row Total |  |  |
| Gender | Girl | O=11E=12.6O=11E=12.6 | O=7E=5.4O=7E=5.4 | _R_ = 18 |
| Boy | O=17E=15.4O=17E=15.4 | O=5E=6.6O=5E=6.6 | _R_ = 22 |  |
| Column Total | _C_ = 28 | _C_ = 12 | _n_ = 40 |  |

A measure of how much the data deviate from what we would expect to see if the factors really were independent is the sum of the squares of the difference of the numbers in each core cell, or, standardizing by dividing each square by the expected number in the cell, the sum Σ\(O−E\)2/E.Σ\(O−E\)2∕E. We would reject the null hypothesis that the factors are independent only if this number is large, so the test is right-tailed. In this example the random variable Σ\(O−E\)2/EΣ\(O−E\)2∕E has the chi-square distribution with one degree of freedom. If we had decided at the outset to test at the 10% level of significance, the critical value defining the rejection region would be, reading from [Figure 12.4 "Critical Values of Chi-Square Distributions"](https://saylordotorg.github.io/text_introductory-statistics/s16-appendix.html), χ2α=χ20.10=2.706χα2=χ0.102=2.706, so that the rejection region would be the interval \[2.706,∞\).\[2.706,∞\). When we compute the value of the standardized test statistic we obtainΣ\(O−E\)2E=\(11−12.6\)212.6+\(7−5.4\)25.4+\(17−15.4\)215.4+\(5−6.6\)26.6=1.231Σ\(O−E\)2E=\(11−12.6\)212.6+\(7−5.4\)25.4+\(17−15.4\)215.4+\(5−6.6\)26.6=1.231

Since 1.231 &lt; 2.706, the decision is not to reject _H_0. See [Figure 11.3 "Baby Gender Prediction"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_f01). The data do not provide sufficient evidence, at the 10% level of significance, to conclude that heart rate and gender are related.

Figure 11.3 Baby Gender Prediction![](https://saylordotorg.github.io/text_introductory-statistics/section_15/a2d445829c9e324c32b00dad5870eb22.jpg)

With this specific example in mind, now turn to the general situation. In the general setting of testing the independence of two factors, call them _Factor 1_ and _Factor 2_, the hypotheses to be tested areH0vs. Ha::The two factors are independentThe two factors are not independentH0:The two factors are independentvs. Ha:The two factors are not independent

As in the example each factor is divided into a number of categories or levels. These could arise naturally, as in the boy-girl division of gender, or somewhat arbitrarily, as in the high-low division of heart rate. Suppose Factor 1 has _I_ levels and Factor 2 has _J_ levels. Then the information from a random sample gives rise to a general _I_ × _J_ contingency table, which with row totals, column totals, and a grand total would appear as shown in [Table 11.3 "General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t03). Each cell may be labeled by a pair of indices \(i,j\).\(i,j\). OijOij stands for the observed count of observations in the cell in row _i_ and column _j_, _Ri_ for the ithith row total and _Cj_ for the jthjth column total. To simplify the notation we will drop the indices so [Table 11.3 "General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t03) becomes [Table 11.4 "Simplified General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t04). Nevertheless it is important to keep in mind that the _O_s, the _R_s and the _C_s, though denoted by the same symbols, are in fact different numbers.

Table 11.3 General Contingency Table

|  | Factor 2 Levels |  |  |  |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 1 |  ⋅ ⋅ ⋅  · · ·  | _j_ |  ⋅ ⋅ ⋅  · · ·  | _J_ | Row Total |  |  |
| Factor 1 Levels | 1 | _O_11 |  ⋅ ⋅ ⋅  · · ·  | O1jO1j |  ⋅ ⋅ ⋅  · · ·  | O1JO1J | _R_1 |
| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |  |
| _i_ | Oi1Oi1 |  ⋅ ⋅ ⋅  · · ·  | OijOij |  ⋅ ⋅ ⋅  · · ·  | OiJOiJ | _Ri_ |  |
| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |  |
| _I_ | OI1OI1 |  ⋅ ⋅ ⋅  · · ·  | OIjOIj |  ⋅ ⋅ ⋅  · · ·  | OIJOIJ | _RI_ |  |
| Column Total | _C_1 |  ⋅ ⋅ ⋅  · · ·  | _Cj_ |  ⋅ ⋅ ⋅  · · ·  | _CJ_ | _n_ |  |

Table 11.4 Simplified General Contingency Table

|  | Factor 2 Levels |  |  |  |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 1 |  ⋅ ⋅ ⋅  · · ·  | _j_ |  ⋅ ⋅ ⋅  · · ·  | _J_ | Row Total |  |  |
| Factor 1 Levels | 1 | _O_ |  ⋅ ⋅ ⋅  · · ·  | _O_ |  ⋅ ⋅ ⋅  · · ·  | _O_ | _R_ |
| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |  |
| _i_ | _O_ |  ⋅ ⋅ ⋅  · · ·  | _O_ |  ⋅ ⋅ ⋅  · · ·  | _O_ | _R_ |  |
| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |  |
| _I_ | _O_ |  ⋅ ⋅ ⋅  · · ·  | _O_ |  ⋅ ⋅ ⋅  · · ·  | _O_ | _R_ |  |
| Column Total | _C_ |  ⋅ ⋅ ⋅  · · ·  | _C_ |  ⋅ ⋅ ⋅  · · ·  | _C_ | _n_ |  |

As in the example, for each core cell in the table we compute what would be the _expected number_ _E_ of observations if the two factors were independent. _E_ is computed for each core cell \(each cell with an _O_ in it\) of [Table 11.4 "Simplified General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t04) by the rule applied in the example:E=R×CnE=R×Cn

where _R_ is the row total and _C_ is the column total corresponding to the cell, and _n_ is the sample size.

After the expected number is computed for every cell, [Table 11.4 "Simplified General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t04) is updated to form [Table 11.5 "Updated General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t05) by inserting the computed value of _E_ into each core cell.

Table 11.5 Updated General Contingency Table

|  | Factor 2 Levels |  |  |  |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 1 |  ⋅ ⋅ ⋅  · · ·  | _j_ |  ⋅ ⋅ ⋅  · · ·  | _J_ | Row Total |  |  |
| Factor 1 Levels | 1 | OEOE |  ⋅ ⋅ ⋅  · · ·  | OEOE |  ⋅ ⋅ ⋅  · · ·  | OEOE | _R_ |
| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |  |
| _i_ | OEOE |  ⋅ ⋅ ⋅  · · ·  | OEOE |  ⋅ ⋅ ⋅  · · ·  | OEOE | _R_ |  |
| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |  |
| _I_ | OEOE |  ⋅ ⋅ ⋅  · · ·  | OEOE |  ⋅ ⋅ ⋅  · · ·  | OEOE | _R_ |  |
| Column Total | _C_ |  ⋅ ⋅ ⋅  · · ·  | _C_ |  ⋅ ⋅ ⋅  · · ·  | _C_ | _n_ |  |

Here is the test statistic for the general hypothesis based on [Table 11.5 "Updated General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t05), together with the conditions that it follow a chi-square distribution.

#### Test Statistic for Testing the Independence of Two Factors

χ2=Σ\(O−E\)2Eχ2=Σ\(O−E\)2E

where the sum is over all core cells of the table.

If

1. the two study factors are independent, and
2. the observed count _O_ of each cell in [Table 11.5 "Updated General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t05) is at least 5,

then χ2χ2 approximately follows a chi-square distribution with df=\(I−1\)×\(J−1\)df=\(I−1\)×\(J−1\) degrees of freedom.

The same five-step procedures, either the critical value approach or the _p_-value approach, that were introduced in [Section 8.1 "The Elements of Hypothesis Testing"](https://saylordotorg.github.io/text_introductory-statistics/fwk-shafer-ch08_s01#fwk-shafer-ch08_s01) and [Section 8.3 "The Observed Significance of a Test"](https://saylordotorg.github.io/text_introductory-statistics/fwk-shafer-ch08_s03#fwk-shafer-ch08_s03) of [Chapter 8 "Testing Hypotheses"](https://saylordotorg.github.io/text_introductory-statistics/s12-testing-hypotheses.html) are used to perform the test, which is always right-tailed.

#### EXAMPLE 1

A researcher wishes to investigate whether students’ scores on a college entrance examination \(CEE\) have any indicative power for future college performance as measured by GPA. In other words, he wishes to investigate whether the factors CEE and GPA are independent or not. He randomly selects _n_ = 100 students in a college and notes each student’s score on the entrance examination and his grade point average at the end of the sophomore year. He divides entrance exam scores into two levels and grade point averages into three levels. Sorting the data according to these divisions, he forms the contingency table shown as [Table 11.6 "CEE versus GPA Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t06), in which the row and column totals have already been computed.

TABLE 11.6 CEE VERSUS GPA CONTINGENCY TABLE

|  | GPA |  |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- |
| &lt;2.7 | 2.7 to 3.2 | &gt;3.2 | Row Total |  |  |
| CEE | &lt;1800&lt;1800 | **35** | **12** | **5** | 52 |
| ≥1800≥1800 | **6** | **24** | **18** | 48 |  |
| Column Total | 41 | 36 | 23 | Total=100Total=100 |  |

Test, at the 1% level of significance, whether these data provide sufficient evidence to conclude that CEE scores indicate future performance levels of incoming college freshmen as measured by GPA.

