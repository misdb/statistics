# 11-1. Chi-Square Tests for Independence

In previous chapters you saw how to test hypotheses concerning population means and population proportions. The idea of testing hypotheses can be extended to many other situations that involve different parameters and use different test statistics. Whereas the standardized test statistics that appeared in earlier chapters followed either a normal or Student _t_-distribution, in this chapter the tests will involve two other very common and useful distributions, the chi-square and the _F_-distributions. 

The **chi-square distribution** arises in tests of hypotheses concerning the _independence of two random variables_ and concerning whether a discrete random variable follows a specified distribution. 

The ****_**F**_**-distribution** arises in tests of hypotheses concerning _whether or not two population variances are equal_ and concerning whether or not three or more population means are equal.

### 11.1 Chi-Square Tests for Independence

#### LEARNING OBJECTIVES

1. To understand what chi-square distributions are.
2. To understand how to use a chi-square test to judge whether two factors are independent.

## 1. Chi-Square Distributions

As you know, there is a whole family of _t_-distributions, each one specified by a parameter called the _degrees of freedom_, denoted $$df$$ . Similarly, all the chi-square distributions form a family, and each of its members is also specified by a parameter $$df$$ , the number of degrees of freedom. Chi is a Greek letter denoted by the symbol $$χ$$ and chi-square is often denoted by $$χ^2$$ . 

[Figure 11.1 "Many "](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s01_f01) shows several chi-square distributions for different degrees of freedom. A chi-square random variable is a random variable that assumes only positive values and follows a chi-square distribution.

Figure 11.1 Many $$χ^2$$ Distributions

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/5a0c7bbacb4242555e8a85c9767c03ee.jpg)

#### 

> _The value of the chi-square random variable_ $$χ^2$$ _with_  $$df=k$$ _that cuts off a right tail of area_ _c_ _is denoted_  $$χ^2_c$$ _and is called a_ **critical value**. _See_ [_Figure 11.2_](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s01_f02)_._ 
>
> Figure 11.2 $$χ^2_c$$  Illustrated __

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/34d06306c2e726f6d5cd7479d9736e5e.jpg)

[Figure 12.4 "Critical Values of Chi-Square Distributions"](https://saylordotorg.github.io/text_introductory-statistics/s16-appendix.html) gives values of $$χ^2_c$$ for various values of _c_ and under several chi-square distributions with various degrees of freedom.

## 2. Tests for Independence

Hypotheses tests encountered earlier in the book had to do with how the numerical values of two population parameters compared. In this subsection we will investigate hypotheses that have to do with whether or not two random variables take their values independently, or whether the value of one has a relation to the value of the other. 

Thus the hypotheses will be expressed in words, not mathematical symbols. We build the discussion around the following example.

There is a theory that the gender of a baby in the womb is related to the baby’s heart rate: baby girls tend to have higher heart rates. Suppose we wish to test this theory. We examine the heart rate records of 40 babies taken during their mothers’ last prenatal checkups before delivery, and to each of these 40 randomly selected records we compute the values of two random measures: 1\) _gender_ and 2\) _heart rate_.

 In this context these two random measures are often called **factors**. Since the burden of proof is that heart rate and gender are related, not that they are unrelated, the problem of testing the theory on baby gender and heart rate can be formulated as a test of the following hypotheses:

             $$H_0:Baby gender and baby heart rate are independent$$   
      vs.  $$H_a:Baby gender and baby heart rate are not independent$$ 

The **factor gender** has two natural categories or levels: _boy and girl_. We divide the **second factor, heart rate,** into two levels, _low and high_, by choosing some heart rate, say 145 beats per minute, as the cutoff between them. A heart rate below 145 beats per minute will be considered low and 145 and above considered high. The 40 records give rise to a 2 × 2 _**contingency table**_**.** 

By adjoining row totals, column totals, and a grand total we obtain the table shown as [Table 11.1 "Baby Gender and Heart Rate"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t01). The four entries in boldface type are counts of observations from the sample of _n_ = 40. There were 11 girls with low heart rate, 17 boys with low heart rate, and so on. They form the _**core**_ of the expanded table.

Table 11.1 Baby Gender and Heart Rate

|  | Heart Rate |  |  |
| :--- | :--- | :--- | :--- |
| Gender | Low | High | Row Total |
| Girl | **11** | **7** | 18 |
| Boy | **17** | **5** | 22 |
| Column Total | 28 | 12 | Total = 40 |

In analogy with the fact that _the probability of independent events is the product of the probabilities of each event_, if heart rate and gender were independent then we would expect the number in each core cell to be close to the product of the row total _**R**_ and column total _**C**_ of the row and column containing it, divided by the sample size _n_. Denoting such an **expected number of observations** _**E**_**,** these four expected values are:

* 1st row and 1st column: $$E=(R×C)∕n=18×28∕40=12.6$$ 
* 1st row and 2nd column: $$E=(R×C)∕n=18×12∕40=5.4$$ 
* 2nd row and 1st column:  $$E=(R×C)∕n=22×28∕40=15.4$$ 
* 2nd row and 2nd column:  $$E=(R×C)∕n=22×12∕40=6.6$$ 

We update [Table 11.1 "Baby Gender and Heart Rate"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t01) by placing each expected value in its corresponding core cell, right under the observed value in the cell. This gives the updated table [Table 11.2 "Updated Baby Gender and Heart Rate"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t02).

Table 11.2 Updated Baby Gender and Heart Rate

|  | Heart Rate |  |  |
| :--- | :--- | :--- | :--- |
| Gender | Low | High | Row Total |
| Girl | O=11 E=12.6 | O=7 E=5.4 | _R_ = 18 |
| Boy | O=17 E=15.4 | O=5 E=6.6 | _R_ = 22 |
| Column Total | _C_ = 28 | _C_ = 12 | _n_ = 40 |

A measure of how much the data deviate from what we would expect to see if the factors really were independent is the sum of the squares of the difference of the numbers in each core cell, or, standardizing by dividing each square by the expected number in the cell, the sum $$Σ(O−E)^2/E$$ . 

**We would reject the null hypothesis** that _the factors are independent_ only if this number is large, so the test is right-tailed. 

In this example the random variable $$Σ(O−E)^2/E$$ has the **chi-square distribution with one degree of freedom**. 

If we had decided at the outset to test at the 10% level of significance, the critical value defining the rejection region would be, reading from [Figure 12.4 "Critical Values of Chi-Square Distributions"](https://saylordotorg.github.io/text_introductory-statistics/s16-appendix.html), $$χ^2_α=χ^2_{0.10}=2.706$$ , so that the rejection region would be the interval $$[2.706,∞)$$ . 

When we compute the value of the standardized test statistic we obtain

     $$Σ\frac{(O−E)^2}{E}=\frac{(11−12.6)^2}{12.6}+\frac{(7−5.4)^2}{5.4}+\frac{(17−15.4)^2}{15.4}+\frac{(5−6.6)^2}{6.6}=1.231$$ 

Since $$1.231 < 2.706$$ , the decision is **not to reject** $$H_0$$ . See [Figure 11.3 "Baby Gender Prediction"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_f01). The data do not provide sufficient evidence, at the 10% level of significance, to conclude that **heart rate and gender are related**.

Figure 11.3 Baby Gender Prediction

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/a2d445829c9e324c32b00dad5870eb22.jpg)

With this specific example in mind, now turn to the **general situation**. 

In the general setting of testing the independence of two factors, call them _**Factor 1**_ ****and _**Factor 2**_**,** the hypotheses to be tested are 

                         $$H_0: The two factors are independent$$   
                  vs.  $$H_a:The two factors are not independent$$ 

As in the example each factor is divided into a number of categories or levels. These could arise naturally, as in the boy-girl division of gender, or somewhat arbitrarily, as in the high-low division of heart rate. 

Suppose Factor 1 has $$I$$ levels and Factor 2 has $$J$$ levels. Then the information from a random sample gives rise to a general $$I × J $$ contingency table, which with row totals, column totals, and a grand total would appear as shown in [Table 11.3 "General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t03). 

Each cell may be labeled by a pair of indices $$(i,j)$$.  $$O_{ij}$$ stands for the observed count of observations in the cell in row _i_ and column _j_, $$R_i $$ for the $$ith$$ row total and $$C_j$$ for the $$jth$$ column total. 

To simplify the notation we will drop the indices so [Table 11.3 "General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t03) becomes [Table 11.4 "Simplified General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t04). Nevertheless it is important to keep in mind that the $$O_s$$ , the $$R_s$$ and the $$C_s$$ , though denoted by the same symbols, are in fact different numbers.

Table 11.3 General Contingency Table

![](../.gitbook/assets/image%20%28185%29.png)

Table 11.4 Simplified General Contingency Table

![](../.gitbook/assets/image%20%2812%29.png)

As in the example, for each core cell in the table we compute what would be the _**expected number**_ ****_**E**_ ****of observations if the two factors were independent. _E_ is computed for each core cell \(each cell with an _O_ in it\) of [Table 11.4 "Simplified General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t04) by the rule applied in the example:

> $$E=R×C_n$$ 
>
> where _R_ is the row total and _C_ is the column total corresponding to the cell, and _n_ is the sample size.

After the expected number is computed for every cell, [Table 11.4 "Simplified General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t04) is updated to form [Table 11.5 "Updated General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t05) by inserting the computed value of _E_ into each core cell.

Table 11.5 Updated General Contingency Table

![](../.gitbook/assets/image%20%284%29.png)

Here is the test statistic for the general hypothesis based on [Table 11.5 "Updated General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t05), together with the conditions that it follow a chi-square distribution.

> **Test Statistic for Testing the Independence of Two Factors** 
>
>                                                   $$χ^2=Σ\frac{(O−E)^2}{E}$$  ****
>
> where the sum is over all core cells of the table. ****
>
> If ****
>
> 1. the two study factors are independent, and
> 2. the observed count _O_ of each cell in [Table 11.5 "Updated General Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t05) is at least 5, 
>
> then $$χ^2$$ approximately follows a chi-square distribution with $$df=(I−1)×(J−1)$$ degrees of freedom.

The same **five-step procedures**, either the **critical value approach** or the _**p**_**-value approach**, that were introduced in [Section 8.1 "The Elements of Hypothesis Testing"](https://saylordotorg.github.io/text_introductory-statistics/fwk-shafer-ch08_s01#fwk-shafer-ch08_s01) and [Section 8.3 "The Observed Significance of a Test"](https://saylordotorg.github.io/text_introductory-statistics/fwk-shafer-ch08_s03#fwk-shafer-ch08_s03) of [Chapter 8 "Testing Hypotheses"](https://saylordotorg.github.io/text_introductory-statistics/s12-testing-hypotheses.html) are used to perform the test, which is always right-tailed.



**EXAMPLE 1.** A researcher wishes to investigate whether students’ scores on a college entrance examination \(CEE\) have any indicative power for future college performance as measured by GPA. In other words, he wishes to investigate whether the factors CEE and GPA are independent or not. He randomly selects _n_ = 100 students in a college and notes each student’s score on the entrance examination and his grade point average at the end of the sophomore year. He divides entrance exam scores into two levels and grade point averages into three levels. Sorting the data according to these divisions, he forms the contingency table shown as [Table 11.6 "CEE versus GPA Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-chi-square-tests-and-f-tests.html#fwk-shafer-ch11_s01_s02_t06), in which the row and column totals have already been computed.

TABLE 11.6 CEE VERSUS GPA CONTINGENCY TABLE

|  | GPA |  |  |  |
| :--- | :--- | :--- | :--- | :--- |
| CEE | &lt;2.7 | 2.7 to 3.2 | &gt;3.2 | Row Total |
| $$<1800$$  | **35** | **12** | **5** | 52 |
| $$≥1800$$  | **6** | **24** | **18** | 48 |
| Column Total | 41 | 36 | 23 | $$Total=100$$  |

Test, at the 1% level of significance, whether these data provide sufficient evidence to conclude that CEE scores indicate future performance levels of incoming college freshmen as measured by GPA.

**\[ Solution \]**

We perform the test using the critical value approach, following the usual five-step method outlined at the end of [Section 8.1 "The Elements of Hypothesis Testing"](https://saylordotorg.github.io/text_introductory-statistics/fwk-shafer-ch08_s01#fwk-shafer-ch08_s01) in [Chapter 8 "Testing Hypotheses"](https://saylordotorg.github.io/text_introductory-statistics/s12-testing-hypotheses.html).

* Step 1. The **hypotheses** are  
                      $$H_0: CEE and GPA are independent factors$$ 

                vs.  $$H_a:CEE and GPA are not independent factors$$ 

* Step 2. The distribution is **chi-square**.
* Step 3. To compute **the value of the test statistic** we must first computed the expected number for each of the six core cells \(the ones whose entries are boldface\):

  * 1st row and 1st column: $$E=(R×C)∕n=41×52∕100=21.32$$ 
  * 1st row and 2nd column: $$E=(R×C)∕n=36×52∕100=18.72$$ 
  * 1st row and 3rd column: $$E=(R×C)∕n=23×52∕100=11.96$$ 
  * 2nd row and 1st column: $$E=(R×C)∕n=41×48∕100=19.68$$ 
  * 2nd row and 2nd column: $$E=(R×C)∕n=36×48∕100=17.28$$ 
  * 2nd row and 3rd column: $$E=(R×C)∕n=23×48∕100=11.04$$ 



  [Table 11.6 "CEE versus GPA Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-01-chi-square-tests-for-independe.html#fwk-shafer-ch11_s01_s02_t06) is updated to [Table 11.7 "Updated CEE versus GPA Contingency Table"](https://saylordotorg.github.io/text_introductory-statistics/s15-01-chi-square-tests-for-independe.html#fwk-shafer-ch11_s01_s02_t07).

*   TABLE 11.7 UPDATED CEE VERSUS GPA CONTINGENCY TABLE

  <table>
    <thead>
      <tr>
        <th style="text-align:left"></th>
        <th style="text-align:left">GPA</th>
        <th style="text-align:left"></th>
        <th style="text-align:left"></th>
        <th style="text-align:left"></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align:left">CEE</td>
        <td style="text-align:left">&lt;2.7</td>
        <td style="text-align:left">2.7 to 3.2</td>
        <td style="text-align:left">&gt;3.2</td>
        <td style="text-align:left">Row Total</td>
      </tr>
      <tr>
        <td style="text-align:left">&lt;1800</td>
        <td style="text-align:left">
          <p>O=35</p>
          <p>E=21.32</p>
        </td>
        <td style="text-align:left">
          <p>O=12</p>
          <p>E=18.72</p>
        </td>
        <td style="text-align:left">
          <p>O=5</p>
          <p>E=11.96</p>
        </td>
        <td style="text-align:left"><em>R</em> = 52</td>
      </tr>
      <tr>
        <td style="text-align:left">&#x2265;1800</td>
        <td style="text-align:left">
          <p>O=6</p>
          <p>E=19.68</p>
        </td>
        <td style="text-align:left">
          <p>O=24</p>
          <p>E=17.28</p>
        </td>
        <td style="text-align:left">
          <p>O=18</p>
          <p>E=11.04</p>
        </td>
        <td style="text-align:left"><em>R=48</em>
        </td>
      </tr>
      <tr>
        <td style="text-align:left">Column Total</td>
        <td style="text-align:left"><em>C</em> = 41</td>
        <td style="text-align:left"><em>C</em> = 36</td>
        <td style="text-align:left"><em>C</em> = 23</td>
        <td style="text-align:left"><em>n</em> = 100</td>
      </tr>
    </tbody>
  </table>The test statistic is   
                      $$χ^2=Σ\frac{(O−E)^2}{E} =\frac{(35−21.32)^2} {21.32}+\frac{(12−18.72)^2} {18.72}+\frac{(5−11.96)^2} {11.96}$$  
                                                          $$+\frac{(6−19.68)^2} {19.68}+\frac{(24−17.28)^2} {17.28}+\frac{(18−11.04)^2} {11.04} = 31.75$$ 

* Step 4. Since the CEE factor has two levels and the GPA factor has three, $$I = 2$$ and $$J = 3$$ . Thus the test statistic follows the **chi-square distribution** with $$df=(2−1)×(3−1)=2$$ degrees of freedom.

  Since the test is **right-tailed**, the critical value is $$χ^2_{0.01}$$ . Reading from [Figure 12.4 "Critical Values of Chi-Square Distributions"](https://saylordotorg.github.io/text_introductory-statistics/s16-appendix.html), $$χ^2_{0.01}=9.210$$ , so the rejection region is $$ [9.210,∞)$$ .

* Step 5. Since 31.75 &gt; 9.21 the decision is to **reject the null hypothesis**. See [Figure 11.4](https://saylordotorg.github.io/text_introductory-statistics/s15-01-chi-square-tests-for-independe.html#fwk-shafer-ch11_s01_s02_f02). The data provide sufficient evidence, at the 1% level of significance, to conclude that CEE score and GPA are not independent: the entrance exam score has predictive power.

Figure 11.4 [Note 11.9 "Example 1"](https://saylordotorg.github.io/text_introductory-statistics/s15-01-chi-square-tests-for-independe.html#fwk-shafer-ch11_s01_s02_n03)

![](https://saylordotorg.github.io/text_introductory-statistics/section_15/68bbab395044eb9d625f2e5d5ad79f81.jpg)

{% tabs %}
{% tab title="R Source" %}
```text
library(Rstat)

# Data 
x <- matrix(c(35, 12, 5, 6, 24, 18), nrow=2, byrow=TRUE)
alpha <- 0.01

# Chi-square Statistic
ct <- chisq.test(x); ct

# Graph of the Chi-square Test
chitest.plot2(stat=ct$stat, df=ct$para, alp=alpha, side="up", pup=0.99999999 )

```
{% endtab %}

{% tab title="Test Statistic" %}
```text
> ct <- chisq.test(x); ct
## 
##         Pearson's Chi-squared test
##
## data:  x
## X-squared = 31.751, df = 2, p-value = 1.275e-07
##
>
```
{% endtab %}

{% tab title="Graph & P-value" %}
![](../.gitbook/assets/image%20%28200%29.png)

```text
> chitest.plot2(stat=ct$stat, df=ct$para, alp=alpha, side="up", pup=0.99999999 )
## P-v = 0 

```
{% endtab %}
{% endtabs %}

## 3. Chi-square Table

![](../.gitbook/assets/image%20%2815%29.png)

