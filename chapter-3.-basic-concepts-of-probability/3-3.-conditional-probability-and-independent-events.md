# 3-3. Conditional Probability and Independent Events

## 1. Conditional Probability

> _The_ conditional probability **of** _A_ **given** _B_, _denoted_ P\(A\|B\)P\(A\|B\), _is the probability that event_ _A_ _has occurred in a trial of a random experiment for which it is known that event_ _B_ _has definitely occurred. It may be computed by means of the following formula:_ 
>
> **Rule for Conditional Probability**    __
>
>                                     $$P(A|B)= \frac{P(A∩B)} {P(B)}$$

**EXAMPLE 21.** A fair die is rolled.

1. Find the probability that the number rolled is a five, given that it is odd.
2. Find the probability that the number rolled is odd, given that it is a five.

**\[ Solution \]**

1. This is the introductory example, so we already know that the answer is 1/3. To use the formula in the definition to confirm this we must replace $$A$$ in the formula \(the event whose likelihood we seek to estimate\) by $$F$$ and replace $$B$$ \(the event we know for certain has occurred\) by $$O$$ :  
         $$P(F|O)=\frac{P(F∩O)}{P(O)}$$ 

   Since  $$F∩O=\{5\}∩\{1,3,5\}=\{5\}, P(F∩O)=1∕6.$$ 

   Since  $$O=\{1,3,5\}, P(O)=3∕6.$$ 

   Thus   $$P(F|O)=\frac{P(F∩O)}{P(O)}=\frac{1∕6}{3∕6}=1/3$$ 

2. This is the same problem, but with the roles of $$F$$ and $$O$$ reversed. Since we are given that the number that was rolled is five, which is odd, the probability in question must be 1. To apply the formula to this case we must now replace _A_ \(the event whose likelihood we seek to estimate\) by $$O$$  and $$B$$ \(the event we know for certain has occurred\) by $$F$$ :  $$P(O|F)=\frac{P(O∩F)}{P(F)}$$  Obviously $$P(F)=1∕6$$ . In part \(a\) we found that $$P(F∩O)=1∕6$$ . Thus  $$P(O|F)=\frac{P(O∩F)}{P(F)}=\frac{1∕6}{1∕6}=1$$ 

{% tabs %}
{% tab title="R Source" %}
```
library(Rstat)

# 0. Sample Space
S <- rolldie2(1); N <- nrow(S)

O <- subset(S, X1 %% 2 == 1) ; element(O); pprt(O, N)
F <- subset(S, X1 == 5); element(F); pprt(F, N)
FO <- intersect2(F, O); pprt(FO, N)

# 1. P(F|O)
pprt(FO,N) / pprt(O,N)       #  P(F∩O)/P(O)
cprt(F,O)                    #  P(F|O)

# 2. P(O|F)
pprt(FO, N) / pprt(F,N)      #  P(F∩O)/P(F)
cprt(O,F)                    #  P(O|F)

```
{% endtab %}

{% tab title="Result" %}
```
> # 0. Sample Space
> S <- rolldie2(1); N <- nrow(S)
> 
> O <- subset(S, X1 %% 2 == 1) ; element(O); pprt(O, N)
## (1) (3) (5) 
## P(O) = 3 / 6 = 0.5 
> F <- subset(S, X1 == 5); element(F); pprt(F, N)
## (5) 
## P(F) = 1 / 6 = 0.1666667 
> FO <- intersect2(F, O); pprt(FO, N)
## P(FO) = 1 / 6 = 0.1666667 
> 
> # 1. P(F|O)
> pprt(FO,N) / pprt(O,N)       #  P(F∩O)/P(O)
## P(FO) = 1 / 6 = 0.1666667 
## P(O) = 3 / 6 = 0.5 
## [1] 0.3333333
> cprt(F,O)                    #  P(F|O)
## P(F|O)= 1 / 3 = 0.3333333 
> 
> # 2. P(O|F)
> pprt(FO, N) / pprt(F,N)      #  P(F∩O)/P(F)
## P(FO) = 1 / 6 = 0.1666667 
## P(F) = 1 / 6 = 0.1666667 
## [1] 1
> cprt(O,F)                    #  P(O|F)
## P(O|F)= 1 / 1 = 1 

```
{% endtab %}
{% endtabs %}



**EXAMPLE 22.** In a sample of 902 individuals under 40 who were or had previously been married, each person was classified according to gender and age at first marriage. The results are summarized in the following two-way classification table, where the meaning of the labels is:

* $$M$$ : male
* $$F$$ : female
* $$E$$ : a teenager when first married
* $$W$$ : in one’s twenties when first married
* $$H$$ : in one’s thirties when first married

|  | $$E$$ | $$W$$ | $$H$$ | Total |
| :--- | :--- | :--- | :--- | :--- |
| $$M$$ | 43 | 293 | 114 | 450 |
| $$F$$ | 82 | 299 | 71 | 452 |
| Total | 125 | 592 | 185 | 902 |

The numbers in the first row mean that 43 people in the sample were men who were first married in their teens, 293 were men who were first married in their twenties, 114 men who were first married in their thirties, and a total of 450 people in the sample were men. Similarly for the numbers in the second row. The numbers in the last row mean that, irrespective of gender, 125 people in the sample were married in their teens, 592 in their twenties, 185 in their thirties, and that there were 902 people in the sample in all. Suppose that the proportions in the sample accurately reflect those in the population of all individuals in the population who are under 40 and who are or have previously been married. Suppose such a person is selected at random.

1. Find the probability that the individual selected was a teenager at first marriage.
2. Find the probability that the individual selected was a teenager at first marriage, given that the person is male.

**\[ Solution \]**

It is natural to let $$E$$ also denote the event that the person selected was a teenager at first marriage and to let $$M$$ denote the event that the person selected is male.

1. According to the table the proportion of individuals in the sample who were in their teens at their first marriage is 125/902. This is the relative frequency of such people in the population, hence $$P(E)=125∕902≈0.139$$ or about 14%.
2. Since it is known that the person selected is male, all the females may be removed from consideration, so that only the row in the table corresponding to men in the sample applies:

   |  | $$E$$ | $$W$$ | $$H$$ | Total |
   | :--- | :--- | :--- | :--- | :--- |
   | $$M$$ | 43 | 293 | 114 | 450 |

The proportion of males in the sample who were in their teens at their first marriage is 43/450. This is the relative frequency of such people in the population of males, hence $$P(E|M)=43∕450≈0.096$$ or about 10%.

{% tabs %}
{% tab title="R Source" %}
```
# 0. Data
sex <- c("M", "F")
married <- c("E", "W", "H")

x <- c(43, 293, 114, 82, 299, 71)
data <- matrix(x, ncol=3, byrow=T)
rownames(data) <- sex
colnames(data) <- married
data

# 1. E : a teenager when first married -> P(E)
E <- sum(data[,"E"]); E
S <- sum(data); S
PE <- E/S; PE

# 2. M : male, P(E|M) = P(E∩M)/P(M)
M <- sum(data["M", ]); M
ME <- data["M", "E"]; ME
ME/M
```
{% endtab %}

{% tab title="Result" %}
```
> data
##    E   W   H
## M 43 293 114
## F 82 299  71
> 
> # 1. E : a teenager when first married -> P(E)
> E <- sum(data[,"E"]); E
## [1] 125
> S <- sum(data); S
## [1] 902
> PE <- E/S; PE
## [1] 0.1385809
> 
> # 2. M : male, P(E|M) = P(E∩M)/P(M)
> M <- sum(data["M", ]); M
## [1] 450
> ME <- data["M", "E"]; ME
## [1] 43
> ME/M
## [1] 0.09555556

```

1. $$P(E)=125∕902≈0.1385809$$
2. $$P(E|M)=43∕450≈0.09555556$$
{% endtab %}
{% endtabs %}



**EXAMPLE 23.** Suppose that in an adult population the proportion of people who are both overweight and suffer hypertension is 0.09; the proportion of people who are not overweight but suffer hypertension is 0.11; the proportion of people who are overweight but do not suffer hypertension is 0.02; and the proportion of people who are neither overweight nor suffer hypertension is 0.78. An adult is randomly selected from this population.

1. Find the probability that the person selected suffers hypertension given that he is overweight.
2. Find the probability that the selected person suffers hypertension given that he is not overweight.
3. Compare the two probabilities just found to give an answer to the question as to whether overweight people tend to suffer from hypertension.

**\[ Solution \]**

Let $$H$$ denote the event “the person selected suffers hypertension.” Let $$O$$ denote the event “the person selected is overweight.” The probability information given in the problem may be organized into the following contingency table:

|  | $$O$$  | \_\_$$O^c$$ __ |
| :--- | :--- | :--- |
| $$H$$  | 0.09 | 0.11 |
| \_\_$$H^c$$ __ | 0.02 | 0.78 |

1. Using the formula in the definition of conditional probability,  $$P(H|O)=\frac{P(H∩O)}{P(O)}=\frac{0.09}{0.09+0.02}=0.8182$$ 
2. Using the formula in the definition of conditional probability, $$P(H|O^c)=\frac{P(H∩O^c)}{P(O^c)}=\frac{0.11}{0.11+0.78}=0.8182$$
3. $$P(H|O)=0.8182$$ is over six times as large as $$P(H|O^c)=0.1236$$ , which indicates a much higher rate of hypertension among people who are overweight than among people who are not overweight. It might be interesting to note that a direct comparison of $$ P(H∩O)=0.09$$ and $$P(H∩O^c)=0.11$$ does not answer the same question.



**Example 24.** In **Example 20**, Find the probabilities of $$P(A|B), P(A|C), P(A|B∩C), P(A|B∪ C)$$ .

**\[ Solution \]**

{% tabs %}
{% tab title="R Source" %}
```
# 1. P(A|B) : cprt(A,B)
cprt(A,B)

# 2. P(A|C) : cprt(A,C)
cprt(A,C)

# 3. P(A|B∩C) : cprt(A, BC)
cprt(A, BC)

# 4. P(A|B∪C) : cprt(A,BuC) 
cprt(A,BuC)
```
{% endtab %}

{% tab title="Result" %}
```
> # 1. P(A|B) : cprt(A,B)
> cprt(A,B)
## P(A|B)= 453 / 671 = 0.6751118 
> 
> # 2. P(A|C) : cprt(A,C)
> cprt(A,C)
## P(A|C)= 140 / 671 = 0.2086438 
> 
> # 3. P(A|B∩C) : cprt(A, BC)
> cprt(A, BC)
## P(A|BC)= 124 / 302 = 0.410596 
> 
> # 4. P(A|B∪C) : cprt(A,BuC) 
> cprt(A,BuC)
## P(A|BuC)= 469 / 1040 = 0.4509615 
```
{% endtab %}
{% endtabs %}



## 2. Independent Events

> _Events_ _A_ _and_ _B_ _are_ **independent** _if_              
>
>                                                     $$P(A∩B)=P(A)⋅P(B)$$   __
>
> _If_ _A_ _and_ _B_ _are not independent then they are_ **dependent**.

\*\*\*\*

**EXAMPLE 25.** A single fair die is rolled. Let $$A=\{3\}$$ and $$B=\{1,3,5\}$$. Are $$A$$ and $$B$$ independent?

**\[ Solution \]**

 In this example we can compute all three probabilities $$P(A)=1∕6, P(B)=1∕2,$$ and $$P(A∩B)=P({3})=1∕6.$$ Since the product $$P(A)⋅P(B)=(1∕6)(1∕2)=1∕12$$ is not the same number as $$P(A∩B)=1∕6,$$ the events _A_ and _B_ are not independent.

\*\*\*\*

**EXAMPLE 26.** The two-way classification of married or previously married adults under 40 according to gender and age at first marriage in "Example 21" produced the table

|  | $$E$$ | $$W$$ | $$H$$ | Total |
| :--- | :--- | :--- | :--- | :--- |
| $$M$$ | 43 | 293 | 114 | 450 |
| $$F$$ | 82 | 299 | 71 | 452 |
| Total | 125 | 592 | 185 | 902 |

Determine whether or not the events _F_: “female” and _E_: “was a teenager at first marriage” are independent.

**\[ Solution \]**

The table shows that in the sample of 902 such adults, 452 were female, 125 were teenagers at their first marriage, and 82 were females who were teenagers at their first marriage, so that  
 $$P(F)=\frac{452}{902} P(E)=\frac{125}{902} P(F∩E)=\frac{82}{902}$$ 

Since  
 $$P(F)⋅P(E)=\frac{452}{902}⋅\frac{125}{902}=0.069$$ 

is not the same as  
 $$P(F∩E)=\frac{82}{902}=0.091$$ 

we conclude that the two events are not independent.

\*\*\*\*

**EXAMPLE 27.** Many diagnostic tests for detecting diseases do not test for the disease directly but for a chemical or biological product of the disease, hence are not perfectly reliable. The _**sensitivity**_ of a test is the probability that the test will be positive when administered to a person who has the disease. The higher the sensitivity, the greater the detection rate and the lower the false negative rate.

Suppose the sensitivity of a diagnostic procedure to test whether a person has a particular disease is 92%. A person who actually has the disease is tested for it using this procedure by two independent laboratories.

1. What is the probability that both test results will be positive?
2. What is the probability that at least one of the two test results will be positive?

**\[ Solution \]**

1. Let $$A_1$$ denote the event “the test by the first laboratory is positive” and let $$A_2$$ denote the event “the test by the second laboratory is positive.” Since $$A_1$$ and $$A_2$$ are independent,  $$P(A_1∩A_2)=P(A_1)⋅P(A_2)=0.92×0.92=0.8464$$ 
2. Using the Additive Rule for Probability and the probability just computed,  $$P(A_1∪A_2)=P(A_1)+P(A_2)−P(A_1∩A_2)=0.92+0.92−0.8464=0.9936$$ 



**EXAMPLE 28**. The _**specificity**_ of a diagnostic test for a disease is the probability that the test will be negative when administered to a person who does not have the disease. The higher the specificity, the lower the false positive rate.

Suppose the specificity of a diagnostic procedure to test whether a person has a particular disease is 89%.

1. A person who does not have the disease is tested for it using this procedure. What is the probability that the test result will be positive?
2. A person who does not have the disease is tested for it by two independent laboratories using this procedure. What is the probability that both test results will be positive?

**\[ Solution \]**

1. Let $$B $$ denote the event “the test result is positive.” The complement of $$B$$ is that the test result is negative, and has probability the specificity of the test, 0.89. Thus  $$P(B)=1−P(B^c)=1−0.89=0.11.$$ 
2. Let $$B_1$$ denote the event “the test by the first laboratory is positive” and let $$B_2$$ denote the event “the test by the second laboratory is positive.” Since $$B_1$$ and $$B_2$$ are independent, by part \(a\) of the example  $$P(B_1∩B_2)=P(B_1)⋅P(B_2)=0.11×0.11=0.0121.$$ 



**EXAMPLE 29.** The reliability of a system can be enhanced by redundancy, which means building two or more independent devices to do the same job, such as two independent braking systems in an automobile.

Suppose a particular species of trained dogs has a 90% chance of detecting contraband in airline luggage. If the luggage is checked three times by three different dogs independently of one another, what is the probability that contraband will be detected?

**\[ Solution \]**

Let $$D_1$$ denote the event that the contraband is detected by the first dog, $$D_2$$ the event that it is detected by the second dog, and $$D_3$$ the event that it is detected by the third. Since each dog has a 90% of detecting the contraband, by the Probability Rule for Complements it has a 10% chance of failing. In symbols, $$P(D^c_1)=0.10,P(D^c_2)=0.10,$$ and $$P(D^c_3)=0.10$$ .

Let _D_ denote the event that the contraband is detected. We seek P\(D\).P\(D\). It is easier to find P\(Dc\)P\(Dc\), because although there are several ways for the contraband to be detected, there is only one way for it to go undetected: all three dogs must fail. Thus $$D^c=D^c_1∩D^c_2∩D^c_3,$$ and $$P(D)=1−P(D^c)=1−P(D^c_1∩D^c_2∩D^c_3)$$ 

But the events $$D_1$$, $$D_2$$, and $$D_3$$ are independent, which implies that their complements are independent, so  
 $$P(D^c_1∩D^c_2∩D^c_3)=P(D^c_1)⋅P(D^c_2)⋅P(D^c_3)=0.10×0.10×0.10=0.001$$ 

Using this number in the previous display we obtain  
 $$P(D)=1−0.001=0.999.$$ 

That is, although any one dog has only a 90% chance of detecting the contraband, three dogs working independently have a 99.9% chance of detecting it.



**Example 30.** Five fair dice are rolled. Let $$A=$${Sum is even} and $$B=$${ It includes 1 and 6 } . Are $$A$$ and $$B$$ independent?

{% tabs %}
{% tab title="R Source" %}
```
library(Rstat)

# 0. Sample Space
S <- rolldie2(5); N <- nrow(S)

# Define the functions
even <- function(x) (sum(x) %% 2 == 0)       # for event A
span5 <- function(x) (max(x) - min(x) == 5)  # for event B

# 1. Event A
A <- subset(S, apply(S, 1, even)); nrow(A)

# 2. Event B
B <- subset(S, apply(S, 1, span5)); nrow(B)

# 3. A and B are independent? => indep.event(A, B, N)
indep.event(A, B, N)
```
{% endtab %}

{% tab title="Result" %}
```
> # 0. Sample Space
> S <- rolldie2(5); N <- nrow(S)
> 
> # Define the functions
> even <- function(x) (sum(x) %% 2 == 0)       # for event A
> span5 <- function(x) (max(x) - min(x) == 5)  # for event B
> 
> # 1. Event A
> A <- subset(S, apply(S, 1, even)); nrow(A)
## [1] 3888
> 
> # 2. Event B
> B <- subset(S, apply(S, 1, span5)); nrow(B)
## [1] 2550
> 
> # 3. A and B are independent? => indep.event(A, B, N)
> indep.event(A, B, N)
## P(A) = 3888 / 7776 = 0.5 
## P(B) = 2550 / 7776 = 0.3279321 
## P(A) × P(B) = 0.163966 
## P(AB) = 1275 / 7776 = 0.163966 
## P(AB) = P(A) × P(B) ⇒ Independent 
## P(A|B) = 1275 / 2550 = 0.5 = P(A) = 3888 / 7776 = 0.5 
## P(B|A) = 1275 / 3888 = 0.3279321 = P(B) = 2550 / 7776 = 0.3279321 
```

* A and B are independent.
{% endtab %}
{% endtabs %}



## 3. Probabilities on Tree Diagrams

**EXAMPLE 31.** A jar contains 10 marbles, 7 black and 3 white. Two marbles are drawn without replacement, which means that the first one is not put back before the second one is drawn.

1. What is the probability that both marbles are black?
2. What is the probability that exactly one marble is black?
3. What is the probability that at least one marble is black?

**\[ Solution \]**

![](https://saylordotorg.github.io/text_introductory-statistics/section_07/d49c86e68633de7735225880cc896d8b.jpg)

{% tabs %}
{% tab title="R Source" %}
```
library(Rstat)

x <- 0:2                    # x : number of black marbles
p_jar <- dhyper(x, 7, 3, 2) # p_jar : P(x)
p_jar

# 1. P(x=2) = p_jar[3]
p_jar[3]

# 2. P(x=1) = p_jar[2]
p_jar[2]

# 3. P(x >=1) = p_jar[2] + p_jar[3]
p_jar[2] + p_jar[3]


```
{% endtab %}

{% tab title="Result" %}
```
> x <- 0:2                    # x : number of black marbles
> p_jar <- dhyper(x, 7, 3, 2) # p_jar : P(x)
> p_jar
## [1] 0.06666667 0.46666667 0.46666667
> 
> # 1. P(x=2) = p_jar[3]
> p_jar[3]
## [1] 0.4666667
> 
> # 2. P(x=1) = p_jar[2]
> p_jar[2]
## [1] 0.4666667
> 
> # 3. P(x >=1) = p_jar[2] + p_jar[3]
> p_jar[2] + p_jar[3]
## [1] 0.9333333
```
{% endtab %}
{% endtabs %}



> **Probabilities on Tree Diagrams**   
>
> 1. The probability of the event corresponding to any node on a tree is the product of the numbers on the unique path of branches that leads to that node from the start.  
> 2. If an event corresponds to several final nodes, then its probability is obtained by adding the numbers next to those nodes.



## 4. Bayes' Theorem

베이즈 정리는 조건부 확률을 직접 계산하기 어려울 때, 조건이 되는 사상을 상호배반 사상들로 분할하여 간접적으로 계산하기 위한 공식이다.

> **전확률 정리 \(Theorem of Total Probability\)**
>
> 표본공간 $$S$$ 를 상호배반\(mutually exclusive\)인 사상 $$A_1, A_2, ..., A_k $$ 로 분할하였을 때, $$S$$ 의 임의의 사상 $$E$$ 에 대하여 아래의 식이 성립한다.
>
> $$P(E) = \Sigma P(A_i ∩E) = \Sigma P(A_i)P(E|A_i)$$



다음의 그림으로부터 사상 E를 다음과 같이 나타낼 수 있다.

 [![](https://mblogthumb-phinf.pstatic.net/MjAxOTAyMDRfNDcg/MDAxNTQ5MjU0OTY5MjU5.IDWt-gpXIhxH71g-haUHKWbL6LroArGiINmhiC8u8vAg.srX7EYx8ei1BwzHVyKM_HHFZ3fJHkdVx6o2pXMZRBKYg.PNG.pmw9440/6.0_%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%881.png?type=w800)](https://m.blog.naver.com/PostView.nhn?blogId=pmw9440&logNo=221457806799&categoryNo=7&proxyReferer=https%3A%2F%2Fwww.google.com%2F#)

* $$P(E) = P(E∩A_1) ∪  P(E∩A_2) ∪P(E∩A_3) ∪...  ∪P(E∩A_k)= \Sigma P(A_i)P(E|A_i)$$ 
* $$P(E∩A_i) = P(A_i∩E) = P(A_i)P(E|A_i)$$  

\*\*\*\*

**Example 32.** 한 제품을 생산하는 공정은 네 개의 생산라인을 가동하고 있다. 각 생산라인별 생산비용과 불량률은 다음의 표와 같다. 이 공정에서 생산된 제품을 한 개 랜덤 샘플링했을 때 불량일 확률을 구하라.

| **생산라인** | A | B | C | D |
| :--- | :--- | :--- | :--- | :--- |
| 생산비율 | 20% | 40% | 30% | 10% |
| 불량률 | 0.04 | 0.02 | 0.01 | 0.05 |

**\[ Solution \]**

제품이 각 생산라인에서 생산된 사상을 A, B, C, D라 하자.   
=&gt; $$P(A)=0.2, P(B) = 0.4, P(C)=0.3, P(D)=0.1$$ 

불량인 사상을 F라 하자.   
=&gt; $$P(F|A) = 0.04, P(F|B)=0.02, P(F|C)=0.01, P(F|D)=0.05$$ 

$$S  = A ∪B∪C∪D$$이고, A, B, C, D는 상호배반\(mutually exclusive\) 사상이다.

$$P(F) = P(F∩A) + P(F∩B) + P(F∩C) + P(F∩D) $$   
             $$=P(A)P(F|A) + P(B)P(F|B) + P(C)P(F|C) + P(D)P(F|D)$$   
             $$= 0.2*0.04 +0.4*0.02 + 0.3 * 0.01 + 0.1*0.05 = 0.024$$ 

{% tabs %}
{% tab title="R Source" %}
```
# production ratio
prior <- c(0.2, 0.4, 0.3, 0.1)

# fault ratio
cond <- c(4, 2, 1, 5) /100

# P(F)
tot <- prior * cond; tot
sum(tot)
```
{% endtab %}

{% tab title="Result" %}
```
> # production ratio
> prior <- c(0.2, 0.4, 0.3, 0.1)
> 
> # fault ratio
> cond <- c(4, 2, 1, 5) /100
> 
> # P(F)
> tot <- prior * cond; tot
## [1] 0.008 0.008 0.003 0.005
> sum(tot)
## [1] 0.024
```

* $$P(F)=0.024$$ 
{% endtab %}
{% endtabs %}



베이즈 정리는 특정 사상 E가 관측되기 이전의 확률로부터 사상 E가 관측된 후의 조건부 확률을 구하기 위한 것이다. 여기서 특정 사상  E가 관측되기 이전의 확률 $$P(A_r)$$ 을 **사전확률\(prior\)**이라 하고, 사상 E가 관측된 후의 조건부 확률 $$P(A_r|E)$$를 **사후확률\(posterior\)**이라 한다.



> **베이스 정리\(Bayes Theorem\)**
>
> 표본공간 $$S$$ 를 공사상이 아닌 \(즉,  $$P(A_i) \ne 0, i=1, 2,...,k$$ \) 사상 $$A_1, A_2,..., A_k$$ 들로 분할하면, 공사상이 아닌 \(즉, $$P(E) \ne 0$$\) 임의의 사상 $$E$$ 에 대하여 아래의 식이 성립한다.
>
> $$P(A_r|E) = \frac {P(A_r∩ E)} {\Sigma P(A_i ∩E)} = \frac{P(A_r)P(E|A_r)}{\Sigma P(A_i)P(E|A_i)}$$



조건부 확률 $$P(A_r | E) = \frac {P(A_r∩E)} {P(E)}$$ 이므로, 분모 $$P(E)$$ 에 전확률 정리를 적용하고, 분자에 곱의 법칙을 적용하면 위의 식을 얻게 된다.



**Example 33.**  Example 32. 에서 불량품이 하나 나왔을 때, 생산라인 $$A, B, C, D$$ 에서 생산되었을 확률을 각각 구하시오.

**\[ Solution \]**

$$P(A|F) = \frac {P(A∩F)}{P(F)} = \frac{0.2 * 0.04}{0.024} = 0.333$$ 

$$P(B|F) = \frac {P(B∩F)}{P(F)}= \frac{0.4 * 0.02}{0.024} = 0.333$$

$$P(C|F) = \frac {P(C∩F)}{P(F)}= \frac{0.3 * 0.01}{0.024} = 0.125$$

$$P(D|F) = \frac {P(D∩F)}{P(F)}= \frac{0.21* 0.05}{0.024}= 0.2083$$

{% tabs %}
{% tab title="R Source" %}
```
library(Rstat)

# production ratio
prior <- c(0.2, 0.4, 0.3, 0.1)

# fault ratio
cond <- c(4, 2, 1, 5) /100

# P(F)
tot <- prior * cond; tot
stot <- sum(tot); stot

# P(A|F), P(B|F), P(C|F), p(D|F)
post <- tot /stot; post

# Draw the plot
bayes.plot(prior, post)

```
{% endtab %}

{% tab title="Result" %}
```
> # production ratio
> prior <- c(0.2, 0.4, 0.3, 0.1)
> 
> # fault ratio
> cond <- c(4, 2, 1, 5) /100
> 
> # P(F)
> tot <- prior * cond; tot
## [1] 0.008 0.008 0.003 0.005
> stot <- sum(tot); stot
## [1] 0.024
> 
> # P(A|F), P(B|F), P(C|F), p(D|F)
> post <- tot /stot; post
## [1] 0.3333333 0.3333333 0.1250000 0.2083333

```
{% endtab %}

{% tab title="Bayes Plot" %}
```
> # Draw the plot
> bayes.plot(prior, post)
```

![](../.gitbook/assets/image%20%28160%29.png)
{% endtab %}
{% endtabs %}

